\subsection{DQN结构}

\begin{frame}{Deep Q-Network 结构}
    代码中配置了 $10$ 台虚拟机，输入状态 $s_t$ 为当前提交任务的类型和以及提交到每台虚拟机时需要等待的时间：
    \[s_t = \left[Type,\, T_{wait}^{(1)},\, T_{wait}^{(2)},\, \dots,\, T_{wait}^{(10)}\right]^T\]

    DQN输出在当前状态 $s_t$ 下，对分配到每台虚拟机的总共 $10$ 个动作的评分。

    \begin{figure}
        \begin{tikzpicture}[scale=0.2,font=\small]
            \draw[->] (-4,0) node[left] {$s_t$} -- (-0.5,0);

            \draw[step=1cm] (0,-4) grid (1,4);
            \node[below] at (0.5,-4) {输入层 $x \in \mathbb{R}^{11}$};

            \draw[->] (1.5,0) -- (16.5,0);
            \node[above right] at (1.5,0) {$ReLU(W_1^Tx+b_1)$};

            \draw[step=1cm] (17,-6) grid (18,6);
            \node[below] at (17.5,-6) {隐藏层 $y \in \mathbb{R}^{20}$};

            \draw[->] (18.5,0) -- (27.5,0);
            \node[above right] at (18.5,0) {$W_2^T y+b_2$};

            \draw[step=1cm] (28,-3) grid (29,3);
            \node[below] at (28.5,-3) {输出层 $q \in \mathbb{R}^{10}$};
        \end{tikzpicture}
    \end{figure}

    其中两层全连接层的参数：$W_1 \in \mathbb{R}^{11 \times 20},\, b_1 \in \mathbb{R}^{20},\, W_2 \in \mathbb{R}^{20 \times 10},\, b_2 \in \mathbb{R}^{10}$ 。
\end{frame}

\subsection{DQN训练方法}

\begin{frame}{Deep Q-Network 训练方法}
    奖励函数：$reward = (1 + e^{\xi - cost}) \cdot \dfrac{T_{exe}}{T_{rep}}$，其中 $\xi$ 为超参数。

    \begin{itemize}
        \item<2-> $\epsilon$-greedy：以 $\epsilon$ 的概率随机决策，以 $1-\epsilon$ 的概率使用DQN决策。随机决策可以探索DQN没有学到的状态，每次学习后减小 $\epsilon$ 。
        \item<3-> experience replay：将过去的决策轨迹 $(s_t, a_t, r_t, s_{t+1})$ 存入replay memory中。DQN每次学习时从replay memory中随机选取一组样本用来更新参数，以消除学习连续样本所带来的相关性。
        \item<4-> fixed Q-target：DQN训练时需要将 $s_t$ 和 $s_{t+1}$ 都输入网络中。如果仅使用一个网络，更新网络参数的操作会使 $s_t$ 和 $s_{t+1}$ 的输出向相同的方向移动。通过引入参数相对固定的 target 网络用来接收 $s_{t+1}$ 的输入后，固定了参数更新的目标，加快了收敛速度。
    \end{itemize}

\end{frame}

\begin{frame}{Deep Q-Network 学习流程}
    确定环境参数：

    \begin{itemize}
        \item 任务：
              \begin{itemize}
                  \item 任务提交速度 $\lambda = 20$
                  \item 任务类型比例 $\text{CPU} : \text{I/O} = 1 : 1$
                  \item 任务平均总计算量 $\mu = 500$
                  \item 任务总计算量的标准差 $\sigma = 20$
                  \item 任务提交总数 $500$
              \end{itemize}
        \item 虚拟机：
              \begin{itemize}
                  \item 虚拟机类型（5台计算型，5台I/O型）$[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]$
                  \item 虚拟机费用 $cost = [1, 1, 2, 2, 4, 1, 1, 2, 2, 4]$
                  \item 单核计算速度 $vCom = 1000$
                  \item 多核加速系数 $vAcc = [1, 1, 1.1, 1.1, 1.2, 1, 1, 1.1, 1.1, 1.2]$
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Deep Q-Network 学习流程}
    确定DRL的超参数：

    \begin{itemize}
        \item DRL超参数：
              \begin{itemize}
                  \item $\epsilon$-greedy $\epsilon = 1.0$ ，每次学习后减小 $0.001$ ，减到 $0.01$ 时不再减小
                  \item replay memory size $N = 100000$
                  \item batch size $S = 256$
                  \item Q-target 网络参数更新间隔 $10$ 步
                  \item 奖励函数超参数 $\xi = 1.5$
                  \item 学习率 $\alpha = 0.001$
                  \item 折扣因子 $\gamma = 0.999$
              \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}{Deep Q-Network 学习流程}
    \scriptsize
    \begin{algorithm}[H]
        \SetAlgoLined
        设定环境参数、DRL超参数，随机初始化DQN参数，赋予target网络相同的参数\;
        \ForEach{Episode}{
            重置环境\;
            \ForEach{Step}{
                对于状态 $s_t$ 根据 $\epsilon$-greedy 策略得到动作 $a_t$\;
                执行动作 $a_t$ 后环境变为 $s_{t+1}$ 并得到奖励 $r_t$\;
                将轨迹 $(s_t, a_t, r_t, s_{t+1})$ 存入replay memory\;
                \If{replay memory 样本数 $\geqslant$ 256}{
                    从replay memory中随机抽取 $256$ 个样本作为 batch\;
                    \ForEach{sample in batch}{
                        将 $s_t$ 传入Q-network得到 $a_t$ 对应的 $Q_{value}$\;
                        将 $s_{t+1}$ 传入target-network得到输出的最大值 $Q_{target}$\;
                        根据损失函数 $Loss = (Q_{value} - (r_t + \gamma \cdot Q_{target}))^2$ 使用梯度下降法更新Q-network\;
                    }
                    \If{Step \% 10 = 0}{
                        使用Q-network的参数更新target-network\;
                    }
                    减小 $\epsilon$\;
                }
            }
        }
    \end{algorithm}
\end{frame}
